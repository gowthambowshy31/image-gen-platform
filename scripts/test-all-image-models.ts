import { GoogleGenerativeAI } from "@google/generative-ai"
import fs from 'fs/promises'
import path from 'path'

const GEMINI_API_KEY = "AIzaSyBL-e8_H0s_HMrmg1WCw41E6NJJ3zcgC-8"

async function testAllImageModels() {
  console.log('üé® Testing ALL Image Generation Capabilities...\n')

  const genAI = new GoogleGenerativeAI(GEMINI_API_KEY)

  // Create output directory
  const outputDir = path.join(process.cwd(), 'public', 'test-outputs')
  await fs.mkdir(outputDir, { recursive: true })

  // List of all potential image generation models
  const imageModels = [
    // Gemini Flash Image models
    { name: 'gemini-2.5-flash-image', method: 'sdk', type: 'Text-to-Image (Free Tier)' },
    { name: 'gemini-2.5-flash-image-preview', method: 'sdk', type: 'Text-to-Image Preview' },
    { name: 'gemini-3-pro-image-preview', method: 'sdk', type: 'Text-to-Image Pro' },
    { name: 'nano-banana-pro-preview', method: 'sdk', type: 'Text-to-Image Nano' },

    // Try with 2.0 flash experimental with image generation flag
    { name: 'gemini-2.0-flash-exp-image-generation', method: 'sdk', type: 'Experimental Image Gen' },
  ]

  const prompt = "A simple red apple on a clean white background, professional product photography style"

  for (const modelInfo of imageModels) {
    console.log('='.repeat(60))
    console.log(`\nüñºÔ∏è  Testing: ${modelInfo.name}`)
    console.log(`   Type: ${modelInfo.type}`)
    console.log(`   Prompt: "${prompt}"\n`)

    try {
      const model = genAI.getGenerativeModel({ model: modelInfo.name })
      const result = await model.generateContent([prompt])
      const response = await result.response

      // Check for image data in response
      let foundImage = false
      if (response.candidates && response.candidates.length > 0) {
        const candidate = response.candidates[0]

        if (candidate.content?.parts) {
          for (const part of candidate.content.parts) {
            if (part.inlineData?.data) {
              // Found image!
              const imageBuffer = Buffer.from(part.inlineData.data, 'base64')
              const filename = `${modelInfo.name.replace(/[./]/g, '-')}_apple_${Date.now()}.png`
              const filepath = path.join(outputDir, filename)

              await fs.writeFile(filepath, imageBuffer)

              console.log('   ‚úÖ SUCCESS! IMAGE GENERATED!')
              console.log(`   üì∏ Saved to: public/test-outputs/${filename}`)
              console.log(`   üìä Size: ${(imageBuffer.length / 1024).toFixed(2)} KB`)
              console.log(`   üéâ THIS MODEL WORKS FOR IMAGE GENERATION!`)

              foundImage = true
              break
            } else if (part.text) {
              // Text response only
              console.log('   ‚ö†Ô∏è  Got text response instead of image:')
              console.log('   ', part.text.substring(0, 150) + '...')
            }
          }
        }
      }

      if (!foundImage) {
        console.log('   ‚ö†Ô∏è  No image data in response')
      }

    } catch (error: any) {
      if (error.message?.includes('429')) {
        console.log('   ‚ùå Rate limited - quota exceeded')
      } else if (error.message?.includes('404')) {
        console.log('   ‚ùå Model not found or not available')
      } else if (error.message?.includes('400')) {
        console.log('   ‚ùå Invalid request or requires billing')
      } else {
        console.log('   ‚ùå Error:', error.message.substring(0, 150))
      }
    }

    // Small delay between requests
    await new Promise(resolve => setTimeout(resolve, 1000))
  }

  // Test vision capability (analyzing images - this SHOULD work)
  console.log('\n' + '='.repeat(60))
  console.log('\nüëÅÔ∏è  BONUS: Testing Vision/Image Analysis (Should Work)...\n')

  try {
    // Create a simple test by describing what an image should look like
    const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" })
    const result = await model.generateContent([
      "Describe in detail what a professional product photo of a red apple should look like for an Amazon listing. Include lighting, background, composition."
    ])
    const response = await result.response
    const text = response.text()

    console.log('‚úÖ Vision/Analysis Working!')
    console.log('\nAI Product Photo Analysis:')
    console.log(text)

  } catch (error: any) {
    console.log('‚ùå Error:', error.message)
  }

  // Summary
  console.log('\n' + '='.repeat(60))
  console.log('üìä FINAL SUMMARY')
  console.log('='.repeat(60))
  console.log('\n‚úÖ Working Features:')
  console.log('   ‚Ä¢ Text generation - WORKING')
  console.log('   ‚Ä¢ Vision/Image analysis - WORKING')
  console.log('   ‚Ä¢ Product descriptions - WORKING')
  console.log('\n‚ö†Ô∏è  Image Generation Status:')
  console.log('   ‚Ä¢ Most models are rate-limited or require billing')
  console.log('   ‚Ä¢ Check public/test-outputs/ for any generated images')
  console.log('\nüí° Next Steps:')
  console.log('   1. Enable billing for full image generation access')
  console.log('   2. Or wait 24 hours for rate limits to reset')
  console.log('   3. Use text/vision features in the meantime')
  console.log('\nüìÅ Output folder: ' + outputDir)
}

testAllImageModels()
